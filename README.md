<div align="center">
<h1>
  T1
</h1>
</div>


<p align="center">
	ğŸ¦‰ <a href="https://github.com/Tele-AI/T1" target="_blank">github</a> â€¢ ğŸ¤— <a href="https://huggingface.co/Tele-AI" target="_blank">Hugging Face</a> â€¢ ğŸ¤– <a href="https://modelscope.cn/organization/TeleAI" target="_blank">ModelScope</a> â€¢ ğŸ¾ <a href="https://gitee.com/Tele-AI/T1" target="_blank">gitee</a> â€¢ ğŸ’¬ <a href="https://github.com/Tele-AI/Telechat/blob/master/images/wechat.jpg" target="_blank">WeChat</a>
</p>

# ç›®å½•

- [æ¨¡å‹ä»‹ç»](#æ¨¡å‹ä»‹ç»)
- [æ•ˆæœè¯„æµ‹](#æ•ˆæœè¯„æµ‹)
- [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
- [å›½äº§åŒ–é€‚é…](#å›½äº§åŒ–é€‚é…)
- [å£°æ˜ã€åè®®ã€å¼•ç”¨](#å£°æ˜åè®®å¼•ç”¨)

# æ¨¡å‹ä»‹ç»

**T1** æ¨¡å‹æ˜¯ **TeleChat** ç³»åˆ—ä¸“æ³¨äºå¤æ‚æ¨ç†çš„æ¨¡å‹ï¼Œç”±ä¸­å›½ç”µä¿¡äººå·¥æ™ºèƒ½ç ”ç©¶é™¢åŸºäºå›½äº§ç®—åŠ›ç ”å‘è®­ç»ƒã€‚è¯¥ç³»åˆ—æ¨¡å‹å€ŸåŠ©å…ˆè¿›çš„æ€ç»´æ¨ç†å’Œæ‰¹åˆ¤çº é”™èƒ½åŠ›ï¼Œåœ¨ä¸‹æ¸¸å¤æ‚ä»»åŠ¡ä¸­æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚æœ¬æ¬¡æˆ‘ä»¬å¼€æºäº† **T1-35B** å’Œ **T1-115B** ä¸¤æ¬¾ä¸åŒå°ºå¯¸çš„æ¨¡å‹ï¼Œä¸åŒå°ºå¯¸æ¨¡å‹ç›¸æ¯”éƒ½å…·æœ‰è¾ƒå¥½çš„æ•ˆæœè¡¨ç°ã€‚

### è®­ç»ƒç­–ç•¥
é‡‡ç”¨è¯¾ç¨‹å­¦ä¹ è´¯ç©¿å…¨æµç¨‹çš„åè®­ç»ƒæ–¹æ¡ˆï¼Œå¾ªåºæ¸è¿›æå‡æ¨¡å‹æ•ˆæœã€‚

- å¾®è°ƒé˜¶æ®µï¼šå°†å¤šä»»åŠ¡æ•°æ®é›†è¿›è¡Œéš¾åº¦åˆ’åˆ†ï¼ˆæ ¹æ®æ¨¡å‹æ¨ç†æ­£è¯¯æ¯”ç‡åˆ¤æ–­ï¼‰ï¼Œé¦–å…ˆä½¿ç”¨ä¸­ä½éš¾åº¦å†·å¯åŠ¨å¾®è°ƒï¼Œç„¶åä½¿ç”¨RFTæ–¹å¼ç­›é€‰ä¸­é«˜éš¾åº¦æ•°æ®è¿›è¡ŒæŒç»­å¾®è°ƒè¿›è¡Œæ•ˆæœæå‡ï¼›
- å¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼šé¦–å…ˆå¯¹æ•°ç†é€»è¾‘ã€ä»£ç èƒ½åŠ›è¿›è¡Œæå‡ï¼Œé‡‡ç”¨éš¾åº¦æ¸è¿›å¼è¯¾ç¨‹å­¦ä¹ æ–¹æ¡ˆè¿›è¡Œèƒ½åŠ›å¼ºåŒ–ï¼›ç„¶åï¼ŒåŸºäºæŒ‡ä»¤éµå¾ªã€å®‰å…¨ã€å¹»è§‰ã€Function Callç­‰10å¤šç§æ··åˆé€šç”¨ä»»åŠ¡è¿›è¡ŒæŒç»­å¼ºåŒ–ï¼Œå…¨é¢æå‡æ¨¡å‹æ•ˆæœï¼›

### æ¨¡å‹ä¸‹è½½
| æ¨¡å‹ç‰ˆæœ¬    | ä¸‹è½½é“¾æ¥                                                      |
|---------|-----------------------------------------------------------|
| T1-35B  | [modelscope](https://modelscope.cn/models/TeleAI/T1-35B)  |
| T1-115B | [modelscope](https://modelscope.cn/models/TeleAI/T1-115B) |

# æ•ˆæœè¯„æµ‹
| æ¨¡å‹                           | MATH-500 | AlignBench | BFCL(avg v1&v2) |
|------------------------------|----------|------------|-----------------|
| OpenAI o1-mini               | 90       | 7.91       | -               |
| DeepSeek-R1-Distill-Qwen-32B | 94.3     | 7.42       | 76.14           |
| QWQ-32B                      | 96       | 7.97       | 83.1            |
| Qwen3-32Bï¼ˆé•¿æ¨ç†ï¼‰               | 93       | 8.27       | 86.82           |
| **T1-35B**                       | 90       | 7.93       | 80.11           |
| **T1-115B**                      | 94       | 8.22       | 83.39           |


# æ¨¡å‹æ¨ç†

### Transformers
T1 ç³»åˆ—æ¨¡å‹æ”¯æŒä½¿ç”¨ `transformers` åº“è¿›è¡Œæ¨ç†ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š


```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("T1/T1-35B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "T1/T1-35B",
    trust_remote_code=True, 
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
prompt = "ç”ŸæŠ½å’Œé…±æ²¹çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(messages,
	tokenize=False,
   	add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```
<details>
<summary>æ¨ç†ç»“æœ</summary>

```markdown

å—¯ï¼Œç”¨æˆ·é—®ç”ŸæŠ½å’Œé…±æ²¹çš„åŒºåˆ«ï¼Œè¿™çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªå…³äºè°ƒå‘³å“çš„é—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®å®šç”¨æˆ·çš„åŸºæœ¬éœ€æ±‚ã€‚å¯èƒ½ä»–ä»¬åœ¨åšé¥­æ—¶ä¸ç¡®å®šè¯¥ç”¨å“ªç§ï¼Œæˆ–è€…æƒ³äº†è§£å®ƒä»¬çš„ç”¨é€”ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘å¾—å›å¿†ä¸€ä¸‹ç”ŸæŠ½å’Œé…±æ²¹çš„å®šä¹‰å’Œç‰¹ç‚¹ã€‚

ç”ŸæŠ½é€šå¸¸æ˜¯æŒ‡ä»¥å¤§è±†å’Œé¢ç²‰ä¸ºåŸæ–™ï¼Œç»è¿‡å‘é…µåˆ¶æˆçš„æ¶²ä½“è°ƒå‘³å“ï¼Œé¢œè‰²è¾ƒæµ…ï¼Œå‘³é“åå’¸é²œã€‚è€Œé…±æ²¹ä¸€èˆ¬æŒ‡çš„æ˜¯è€æŠ½ï¼Œé¢œè‰²æ·±ï¼Œå‘³é“æ›´æµ“éƒï¼Œå¸¸ç”¨äºä¸Šè‰²ã€‚ä¸è¿‡ï¼Œå¯èƒ½ç”¨æˆ·å¯¹â€œé…±æ²¹â€çš„ç†è§£æœ‰åå·®ï¼Œå› ä¸ºæœ‰äº›åœ°æ–¹å¯èƒ½æŠŠé…±æ²¹æ³›æŒ‡æ‰€æœ‰é…±æ²¹ç±»è°ƒå‘³å“ï¼ŒåŒ…æ‹¬ç”ŸæŠ½å’Œè€æŠ½ã€‚

ç„¶åï¼Œæˆ‘éœ€è¦è€ƒè™‘ç”¨æˆ·çš„èƒŒæ™¯ã€‚å¯èƒ½æ˜¯å®¶åº­çƒ¹é¥ªè€…ï¼Œå°¤å…¶æ˜¯æ–°æ‰‹ï¼Œå¯¹è°ƒæ–™ä¸å¤ªç†Ÿæ‚‰ã€‚ä»–ä»¬å¯èƒ½æƒ³çŸ¥é“åœ¨ç‚’èœæˆ–å‡‰æ‹Œæ—¶è¯¥ç”¨å“ªç§ï¼Œæˆ–è€…å¦‚ä½•æ­£ç¡®ä½¿ç”¨ä»¥è¾¾åˆ°æœ€ä½³æ•ˆæœã€‚æ­¤å¤–ï¼Œç”¨æˆ·å¯èƒ½æ²¡æœ‰æ˜ç¡®åŒºåˆ†ç”ŸæŠ½ã€è€æŠ½å’Œå…¶ä»–ç±»å‹çš„é…±æ²¹ï¼Œæ¯”å¦‚å‘³æé²œã€è’¸é±¼è±‰æ²¹ç­‰ã€‚

æ¥ä¸‹æ¥ï¼Œè¦åˆ†æç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ã€‚é™¤äº†åŸºæœ¬çš„åŒºåˆ«ï¼Œä»–ä»¬å¯èƒ½è¿˜å…³å¿ƒä½¿ç”¨åœºæ™¯ã€å¥åº·å› ç´ ï¼ˆæ¯”å¦‚ç›åˆ†å«é‡ï¼‰æˆ–è€…å¦‚ä½•æ›¿ä»£ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œç”ŸæŠ½é€‚åˆå‡‰æ‹Œå’Œç‚’èœï¼Œè€Œè€æŠ½é€‚åˆçº¢çƒ§å’Œå¤å‘³ã€‚å¦‚æœç”¨æˆ·æœ‰å¥åº·é¡¾è™‘ï¼Œå¯èƒ½éœ€è¦æåˆ°ä½ç›äº§å“ã€‚

è¿˜è¦è€ƒè™‘å¸¸è§çš„è¯¯åŒºã€‚æ¯”å¦‚ï¼Œæœ‰äº›äººå¯èƒ½è®¤ä¸ºé…±æ²¹å°±æ˜¯ç”ŸæŠ½ï¼Œæˆ–è€…æ··æ·†äº†ç”ŸæŠ½å’Œè€æŠ½çš„åŠŸèƒ½ã€‚éœ€è¦æ˜ç¡®æŒ‡å‡ºä¸¤è€…çš„ä¸åŒç”¨é€”ï¼Œé¿å…ç”¨æˆ·åœ¨çƒ¹é¥ªæ—¶å‡ºé”™ã€‚

å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½å¯¹â€œé…±æ²¹â€è¿™ä¸ªè¯çš„å¹¿ä¹‰ç†è§£æœ‰è¯¯ï¼Œè®¤ä¸ºæ‰€æœ‰é…±æ²¹éƒ½æ˜¯åŒä¸€ç§ä¸œè¥¿ã€‚è¿™æ—¶å€™éœ€è¦è§£é‡Šæ¸…æ¥šï¼Œé…±æ²¹æ˜¯ä¸€ä¸ªå¤§ç±»ï¼ŒåŒ…å«å¤šç§ç±»å‹ï¼Œè€Œç”ŸæŠ½æ˜¯å…¶ä¸­çš„ä¸€ç§ï¼Œä¸»è¦ç”¨äºæé²œï¼Œè€æŠ½ç”¨äºä¸Šè‰²ã€‚

æœ€åï¼Œæ€»ç»“å›ç­”æ—¶è¦ç»“æ„æ¸…æ™°ï¼Œåˆ†ç‚¹è¯´æ˜åŒºåˆ«ï¼ŒåŒ…æ‹¬åŸæ–™ã€é¢œè‰²ã€å‘³é“ã€ç”¨é€”å’Œå¥åº·æç¤ºã€‚ç¡®ä¿è¯­è¨€é€šä¿—æ˜“æ‡‚ï¼Œé¿å…ä¸“ä¸šæœ¯è¯­è¿‡å¤šï¼Œè®©ç”¨æˆ·å®¹æ˜“ç†è§£ã€‚åŒæ—¶ï¼Œæé†’ç”¨æˆ·æ ¹æ®çƒ¹é¥ªéœ€æ±‚é€‰æ‹©åˆé€‚çš„è°ƒæ–™ï¼Œå¯èƒ½è¿˜éœ€è¦ä¸¾ä¾‹è¯´æ˜å¸¸è§èœå“ä¸­ä½¿ç”¨çš„æƒ…å†µï¼Œå¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°åº”ç”¨çŸ¥è¯†ã€‚
</think>

ç”ŸæŠ½å’Œé…±æ²¹éƒ½æ˜¯å¸¸ç”¨çš„è°ƒå‘³å“ï¼Œä½†å®ƒä»¬åœ¨åŸæ–™ã€åˆ¶ä½œå·¥è‰ºã€é£å‘³å’Œç”¨æ³•ä¸Šæœ‰æ˜æ˜¾åŒºåˆ«ã€‚ä»¥ä¸‹æ˜¯è¯¦ç»†çš„åŒºåˆ†ï¼š

---

### **1. åŸæ–™ä¸å·¥è‰º**
- **ç”ŸæŠ½**  
  - **åŸæ–™**ï¼šä»¥å¤§è±†ï¼ˆé»„è±†æˆ–é»‘è±†ï¼‰ã€å°éº¦ç²‰ä¸ºä¸»ï¼Œè¾…ä»¥ç›ã€æ°´ã€æ›²ç§ï¼ˆç±³æ›²éœ‰ï¼‰ã€‚  
  - **å·¥è‰º**ï¼šé€šè¿‡å‘é…µï¼ˆå›ºæ€æˆ–æ¶²æ€ï¼‰åˆ¶æˆï¼Œé¢œè‰²è¾ƒæµ…ï¼Œå‘ˆçº¢è¤è‰²æˆ–ç¥ç€è‰²ï¼Œå‘³é“åå’¸é²œï¼Œå«è¾ƒå¤šæ°¨åŸºé…¸ã€‚  

- **é…±æ²¹**ï¼ˆä¼ ç»Ÿå¹¿å¼è€é…±æ²¹æˆ–è€æŠ½ï¼‰  
  - **åŸæ–™**ï¼šåŒæ ·ä»¥å¤§è±†ã€å°éº¦ä¸ºåŸºç¡€ï¼Œä½†å¯èƒ½æ·»åŠ ç„¦ç³–è‰²ã€ç³–ç±»æˆ–å…¶ä»–å¢è‰²æˆåˆ†ã€‚  
  - **å·¥è‰º**ï¼šå‘é…µåéœ€é•¿æ—¶é—´æ™¾æ™’æˆ–çƒ˜çƒ¤ï¼Œé¢œè‰²æ›´æ·±ï¼ˆæ·±è¤è‰²ï¼‰ï¼Œå’¸å‘³è¾ƒé‡ï¼Œé²œå‘³ç•¥é€Šäºç”ŸæŠ½ã€‚

---

### **2. é£å‘³å·®å¼‚**
- **ç”ŸæŠ½**ï¼šé²œå‘³çªå‡ºï¼Œå’¸ä¸­å¸¦ç”œï¼Œé€‚åˆç›´æ¥è°ƒå‘³ã€‚  
- **é…±æ²¹**ï¼šå’¸å‘³æµ“éƒï¼Œéƒ¨åˆ†äº§å“å¸¦æœ‰ç„¦ç³–é¦™æ°”ï¼Œå¸¸ç”¨äºä¸Šè‰²æˆ–æå‘³ã€‚

---

### **3. ç”¨é€”åŒºåˆ«**
- **ç”ŸæŠ½**ï¼š  
  - å‡‰æ‹Œèœï¼ˆå¦‚æ‹Œé»„ç“œã€æµ·å¸¦ï¼‰å¢é²œæå‘³ã€‚  
  - ç‚’èœï¼ˆå¦‚å°ç‚’è‚‰ã€é’èœï¼‰ä»£æ›¿ç›ï¼Œä¿æŒé£ŸæåŸå‘³ã€‚  
  - è˜¸æ–™ï¼ˆå¦‚ç™½åˆ‡é¸¡ã€åˆºèº«ï¼‰æé²œã€‚  

- **é…±æ²¹**ï¼š  
  - çº¢çƒ§ã€ç‚–ç…®ï¼ˆå¦‚çº¢çƒ§è‚‰ã€å¤å‘³ï¼‰ä¸Šè‰²å’Œå¢é¦™ã€‚  
  - è…Œåˆ¶æˆ–è˜¸é£Ÿï¼ˆå¦‚è˜¸é¥ºå­ã€æ‹Œé¢ï¼‰å¢åŠ å’¸é¦™ã€‚

---

### **4. å¥åº·æ³¨æ„äº‹é¡¹**
- **ç›åˆ†**ï¼šä¸¤è€…å«ç›é‡è¾ƒé«˜ï¼Œç”ŸæŠ½é’ å«é‡çº¦ä¸º15%ï¼ˆæ¯ç™¾æ¯«å‡ï¼‰ï¼Œé…±æ²¹çº¦18%ï¼Œéœ€æ³¨æ„æ§åˆ¶ç”¨é‡ã€‚  
- **æ·»åŠ å‰‚**ï¼šéƒ¨åˆ†ç”ŸæŠ½å«è°·æ°¨é…¸é’ ï¼ˆå‘³ç²¾ï¼‰ï¼Œå¢åŠ é²œå‘³ï¼›é…±æ²¹å¯èƒ½æ·»åŠ ç„¦ç³–è‰²ç´ æˆ–é˜²è…å‰‚ï¼Œé€‰æ‹©æ—¶å¯å‚è€ƒé…æ–™è¡¨ã€‚

---

### **å¸¸è§è¯¯åŒºæ¾„æ¸…**
- **â€œé…±æ²¹=è€æŠ½â€**ï¼šä¼ ç»Ÿå¹¿å¼é…±æ²¹ï¼ˆå¦‚è±‰æ²¹ï¼‰ä¸è€æŠ½ï¼ˆåŒ—æ–¹ç§°æ³•ï¼‰ç±»ä¼¼ï¼Œä½†éƒ¨åˆ†åœ°åŒºå°†â€œé…±æ²¹â€æ³›æŒ‡ç‚¹ç¼€æ‰€æœ‰é…±æ²¹ç±»ï¼ˆåŒ…æ‹¬ç”ŸæŠ½ã€è€æŠ½ã€å‘³æé²œç­‰ï¼‰ã€‚è´­ä¹°æ—¶éœ€çœ‹æ¸…åŒ…è£…æ ‡æ³¨ã€‚  
- **æ›¿ä»£ä½¿ç”¨**ï¼šç”ŸæŠ½ä¸å¯å®Œå…¨æ›¿ä»£é…±æ²¹ï¼Œå› ä¸Šè‰²èƒ½åŠ›ä¸è¶³ï¼›è€æŠ½å’¸å‘³è¿‡é‡ï¼Œä¸å®œç›´æ¥ç”¨äºå‡‰æ‹Œã€‚

---

### **æ€»ç»“**
| **å¯¹æ¯”é¡¹**   | **ç”ŸæŠ½**                | **é…±æ²¹ï¼ˆè€æŠ½ï¼‰**          |
|--------------|-------------------------|--------------------------|
| **é¢œè‰²**     | æ·¡çº¢è¤è‰²                | æ·±è¤è‰²                  |
| **é²œå‘³**     | çªå‡º                    | è¾ƒå¼±                    |
| **å’¸å‘³**     | ä¸­ç­‰                    | è¾ƒæµ“                    |
| **ä¸»è¦ç”¨é€”** | å‡‰æ‹Œã€ç‚’èœã€è˜¸æ–™        | ä¸Šè‰²ã€ç‚–ç…®ã€è…Œåˆ¶        |

æ ¹æ®çƒ¹é¥ªéœ€æ±‚é€‰æ‹©ï¼šè¿½æ±‚é²œå‘³ç”¨ç”ŸæŠ½ï¼Œéœ€è¦ä¸Šè‰²æˆ–æµ“éƒå’¸é¦™ç”¨é…±æ²¹ã€‚
```
</details>

#### ModelScope
T1 ç³»åˆ—æ¨¡å‹æ”¯æŒä½¿ç”¨ ModelScope æ¨ç†ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
import os
import torch
from modelscope import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
tokenizer = AutoTokenizer.from_pretrained('T1/T1-35BB', trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained('T1/T1-35B', trust_remote_code=True, device_map="auto",
                                                  torch_dtype=torch.bfloat16)
prompt = "ç”ŸæŠ½ä¸è€æŠ½çš„åŒºåˆ«ï¼Ÿ"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]
response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

### vLLM æ¨ç†

T1 æ”¯æŒä½¿ç”¨ [vLLM](https://github.com/vllm-project/vllm) è¿›è¡Œéƒ¨ç½²ä¸æ¨ç†åŠ é€Ÿï¼Œç¤ºä¾‹å¦‚ä¸‹:
##### ç¦»çº¿æ¨ç†
```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

tokenizer = AutoTokenizer.from_pretrained("T1/T1-35B", trust_remote_code=True)
sampling_params = SamplingParams(temperature=0.6, repetition_penalty=1.05, max_tokens=8192)
llm = LLM(model="T1/T1-35B", trust_remote_code=True, tensor_parallel_size=4, dtype="bfloat16")

prompt = "ç”ŸæŠ½å’Œé…±æ²¹çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"
messages = [{"role": "user", "content": prompt}]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

outputs = llm.generate([text], sampling_params)
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

##### OpenAI å…¼å®¹çš„ API æœåŠ¡
æ‚¨å¯ä»¥å€ŸåŠ© vLLMï¼Œæ„å»ºä¸€ä¸ªä¸ OpenAI API å…¼å®¹çš„ API æœåŠ¡ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ‰€ç¤ºè¿è¡Œå‘½ä»¤ï¼š
```
vllm serve T1/T1-35B \
    --trust-remote-code \
    --dtype bfloat16 \
    --disable-custom-all-reduce
```

ç„¶åï¼Œæ‚¨å¯ä»¥ä¸ T1 è¿›è¡Œå¯¹è¯ï¼š
```python
from openai import OpenAI
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(api_key=openai_api_key, base_url=openai_api_base)
chat_response = client.chat.completions.create(
    model="T1/T1-35B",
    messages=[
        {"role": "user", "content": "ç”ŸæŠ½å’Œé…±æ²¹çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"},
    ],
    temperature=0.6,
    max_tokens=8192,
    extra_body={
        "repetition_penalty": 1.05,
        "skip_special_tokens": False,
        "spaces_between_special_tokens": False,
    },
)
print("Chat response:", chat_response)
```


#### æ¨ç†æ³¨æ„äº‹é¡¹

1. T1 ç³»åˆ—æ¨¡å‹åœ¨ chat template ä¸­åŠ å…¥äº†ä¸€äº›é€‚é…å¤æ‚æ¨ç†æ¨¡å‹çš„ç‰¹æ€§ï¼š
    - T1 ç³»åˆ—æ¨¡å‹åœ¨ chat template ä¸­åŠ å…¥äº†`<think>\n`ç¬¦å·ä»¥ç¡®ä¿æ¨ç†æ—¶èƒ½å¤Ÿç”Ÿæˆ reason è¿‡ç¨‹ã€‚å¦‚æœå€ŸåŠ© `transformers` åº“æ¨ç†ï¼Œå¹¶é‡‡ç”¨`apply_chat_template`æ–¹æ³•ï¼Œä¸” `add_generation_prompt` è®¾ä¸º`True`ï¼Œåˆ™å°†ä¼šåœ¨æ¨ç†æ—¶è‡ªåŠ¨æ‹¼æ¥`<think>\n`ç¬¦å·ï¼›å¦‚æœä½¿ç”¨ vLLM åº“æ¨ç†ï¼Œä¹Ÿä¼šè‡ªåŠ¨åœ¨æ¨ç†èµ·å§‹æ‹¼æ¥`<think>\n`ç¬¦å·ã€‚æ­¤æ—¶è¾“å‡ºç»“æœä¼šç¼ºå°‘å¼€å¤´çš„`<think>\n`ç¬¦å·ã€‚
    - T1 ç³»åˆ—æ¨¡å‹åœ¨è¿›è¡Œå¤šè½®æ¨ç†æ—¶ä¸åº”ä¼ å…¥ä¹‹å‰è½®æ¬¡å›ç­”ä¸­çš„`<think>..</think>`è¿‡ç¨‹ï¼Œåœ¨chat template ä¸­å·²ç»å®ç°äº†å¯¹å¤šè½®å†å²ä¿¡æ¯çš„è‡ªåŠ¨å¤„ç†ã€‚

2. T1 ç³»åˆ—æ¨¡å‹æ¨ç†å‚æ•°é€‰æ‹©
    - åœ¨æ¨ç†æ•°å­¦ã€ä»£ç ä»»åŠ¡æ—¶ï¼Œå»ºè®®ä½¿ç”¨`repetition_penalty=1.0, temperature=0.6, top_p=0.95`çš„æ¨ç†è®¾ç½®ã€‚
    - åœ¨æ¨ç†é€šç”¨ä»»åŠ¡æ—¶ï¼Œå»ºè®®ä½¿ç”¨`repetition_penalty=1.05, temperature=0.6, top_p=0.95`çš„æ¨ç†è®¾ç½®ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘é‡å¤ç”Ÿæˆç°è±¡ã€‚


# å›½äº§åŒ–é€‚é…

T1ç³»åˆ—æ¨¡å‹å‡è¿›è¡Œäº†**å›½äº§åŒ–ç®—åŠ›é€‚é…**ï¼Œå…·ä½“ä¿¡æ¯å¯è§
1. <a href="https://modelers.cn/models/MindSpore-Lab/T1-35B" target="_blank">MindSpore-Lab/T1-35B</a>
2. <a href="https://modelers.cn/models/MindSpore-Lab/T1-115B" target="_blank">MindSpore-Lab/T1-115B</a>


# å£°æ˜ã€å¼•ç”¨

### å£°æ˜

æˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œä¸è¦ä½¿ç”¨ T1 ç³»åˆ—æ¨¡å‹åŠå…¶è¡ç”Ÿæ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°† T1 ç³»åˆ—æ¨¡å‹ç”¨äºæ²¡æœ‰å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰ä½¿ç”¨è€…éµå®ˆä¸Šè¿°åŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€å‘å±•åœ¨åˆæ³•åˆè§„çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚

æˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨ T1 ç³»åˆ—å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

### å¼•ç”¨

å¦‚éœ€å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·ä½¿ç”¨å¦‚ä¸‹ referenceï¼š

```
@misc{wang2025technicalreporttelechat2telechat25,
      title={Technical Report of TeleChat2, TeleChat2.5 and T1}, 
      author={Zihan Wang and Xinzhang Liu and Yitong Yao and Chao Wang and Yu Zhao and Zhihao Yang and Wenmin Deng and Kaipeng Jia and Jiaxin Peng and Yuyao Huang and Sishi Xiong and Zhuo Jiang and Kaidong Yu and Xiaohui Hu and Fubei Yao and Ruiyu Fang and Zhuoru Jiang and Ruiting Song and Qiyi Xie and Rui Xue and Xuewei He and Yanlei Xue and Zhu Yuan and Zhaoxi Zhang and Zilu Huang and Shiquan Wang and Xin Wang and Hanming Wu and Mingyuan Wang and Xufeng Zhan and Yuhan Sun and Zhaohu Xing and Yuhao Jiang and Bingkai Yang and Shuangyong Song and Yongxiang Li and Zhongjiang He and Xuelong Li},
      year={2025},
      eprint={2507.18013},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.18013}, 
}
```

